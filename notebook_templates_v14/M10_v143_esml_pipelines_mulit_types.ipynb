{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB! This,InteractiveLoginAuthentication, is only needed to run 1st time, then when ws_config is written, use later CELL in notebook, that just reads that file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../azure-enterprise-scale-ml/esml/common/\")\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from esml import ESMLProject\n",
    "\n",
    "p = ESMLProject()\n",
    "auth = InteractiveLoginAuthentication(tenant_id = p.tenant)\n",
    "ws, config_name = p.authenticate_workspace_and_write_config(auth)\n",
    "p.ws = ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd, 3rd this - run thic CELL below. \n",
    "- To attach ESML controlplane to your project\n",
    "- To point at `template-data` for the pipelinbe to know the schema of data\n",
    "- To init the ESMLPieplinefactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using lake_settings.json with ESML version 1.4 - Models array support including LABEL\n",
      "\n",
      " ---- Q: WHICH files are generated as templates, for you to EDIT? ---- \n",
      "A: These files & locations:\n",
      "File to EDIT (step: IN_2_SILVER_1): ../../../2_A_aml_pipeline/4_inference/batch/M10/in2silver_ds01_titanic.py\n",
      "File to EDIT (step: IN_2_SILVER_2): ../../../2_A_aml_pipeline/4_inference/batch/M10/in2silver_ds02_haircolor.py\n",
      "File to EDIT (step: IN_2_SILVER_3): ../../../2_A_aml_pipeline/4_inference/batch/M10/in2silver_ds03_housing.py\n",
      "File to EDIT (step: IN_2_SILVER_4): ../../../2_A_aml_pipeline/4_inference/batch/M10/in2silver_ds04_lightsaber.py\n",
      "File to EDIT (step: SILVER_MERGED_2_GOLD): ../../../2_A_aml_pipeline/4_inference/batch/M10/silver_merged_2_gold.py\n",
      "File to EDIT (step: SCORING_GOLD): ../../../2_A_aml_pipeline/4_inference/batch/M10/scoring_gold.py\n",
      "File to EDIT (step: TRAIN_SPLIT_AND_REGISTER): ../../../2_A_aml_pipeline/4_inference/batch/M10/train_split_and_register.py\n",
      "File to EDIT (step: TRAIN_MANUAL): ../../../2_A_aml_pipeline/4_inference/batch/M10/train_manual.py\n",
      "File to EDIT a lot (reference in step-scripts Custom code): ../../../2_A_aml_pipeline/4_inference/batch/M10/your_code/your_custom_code.py\n",
      "\n",
      " ---- WHAT model to SCORE with, & WHAT data 'date_folder'? ---- \n",
      "InferenceModelVersion (model version to score with): 1\n",
      "Date_scoring_folder (data to score) : 1000-01-01 10:35:01.243860\n",
      "ESML environment: dev\n",
      "Inference mode (self.batch_pipeline_parameters[4]): 1\n",
      "\n",
      " ---- ESML Datalake locations: ESML Datasets (IN-data) ---- \n",
      "Name (lake folder): ds01_titanic and AzureName IN: M10_ds01_titanic_inference_IN\n",
      "IN projects/project002/10_titanic_model_clas/inference/1/ds01_titanic/in/dev/2021/06/08/\n",
      "Bronze projects/project002/10_titanic_model_clas/inference/1/ds01_titanic/out/bronze/dev/\n",
      "Silver projects/project002/10_titanic_model_clas/inference/1/ds01_titanic/out/silver/dev/\n",
      "\n",
      "Name (lake folder): ds02_haircolor and AzureName IN: M10_ds02_haircolor_inference_IN\n",
      "IN projects/project002/10_titanic_model_clas/inference/1/ds02_haircolor/in/dev/2021/06/08/\n",
      "Bronze projects/project002/10_titanic_model_clas/inference/1/ds02_haircolor/out/bronze/dev/\n",
      "Silver projects/project002/10_titanic_model_clas/inference/1/ds02_haircolor/out/silver/dev/\n",
      "\n",
      "Name (lake folder): ds03_housing and AzureName IN: M10_ds03_housing_inference_IN\n",
      "IN projects/project002/10_titanic_model_clas/inference/1/ds03_housing/in/dev/2021/06/08/\n",
      "Bronze projects/project002/10_titanic_model_clas/inference/1/ds03_housing/out/bronze/dev/\n",
      "Silver projects/project002/10_titanic_model_clas/inference/1/ds03_housing/out/silver/dev/\n",
      "\n",
      "Name (lake folder): ds04_lightsaber and AzureName IN: M10_ds04_lightsaber_inference_IN\n",
      "IN projects/project002/10_titanic_model_clas/inference/1/ds04_lightsaber/in/dev/2021/06/08/\n",
      "Bronze projects/project002/10_titanic_model_clas/inference/1/ds04_lightsaber/out/bronze/dev/\n",
      "Silver projects/project002/10_titanic_model_clas/inference/1/ds04_lightsaber/out/silver/dev/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../azure-enterprise-scale-ml/esml/common/\")\n",
    "from esml import ESMLProject\n",
    "from baselayer_azure_ml_pipeline import ESMLPipelineFactory, esml_pipeline_types\n",
    "\n",
    "p = ESMLProject()\n",
    "p.inference_mode = True\n",
    "p.active_model = 10 # 10=titanic , 11=Diabetes\n",
    "p_factory = ESMLPipelineFactory(p)\n",
    "\n",
    "scoring_date = '1000-01-01 10:35:01.243860' # \n",
    "p_factory.batch_pipeline_parameters[1].default_value = scoring_date # overrides ESMLProject.date_scoring_folder.\n",
    "p_factory.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN_2_GOLD_SCORING\n",
    "- Full ML-workflow: If you want to refine data from IN to GOLD, and SCORE model on GOLD, saves SCORED_GOLD in datalake\n",
    "- Scenario: You want MLOps and full automation, ESMLPipelineFactory starting from Azure Data factory, and calling this genereated Azure ML Pipeline. \n",
    "    - Pipeline saving data automatically using the enterprise datalake/ESML AutoLake and ESML SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BUILD\n",
    "p_factory.create_dataset_scripts_from_template(overwrite_if_exists=True) # Do this once, then edit them manually. overwrite_if_exists=False is DEFAULT\n",
    "batch_pipeline = p_factory.create_batch_pipeline(esml_pipeline_types.IN_2_GOLD_SCORING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN\n",
    "pipeline_run = p_factory.execute_pipeline(batch_pipeline)\n",
    "pipeline_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUBLISH\n",
    "published_pipeline, endpoint = p_factory.publish_pipeline(batch_pipeline,\"_1\") # \"_1\" is optional    to create a NEW pipeline with 0 history, not ADD version to existing pipe & endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"In AZURE DATA FACTORY - This is the ID you need, if using PRIVATE LINK, private Azure ML workspace.\")\n",
    "print(\"-You need PIPELINE id, not pipeline ENDPOINT ID ( since cannot be chosen in Azure data factory if private Azure ML)\")\n",
    "published_pipeline.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN_2_GOLD  only (for scoring/inference purpose)\n",
    "- If just wanting to refine data to GOLD, before SCORE step\n",
    "- Scenario: You want to refine data from \"IN_2_GOLD\" with an easy way to READ/WRITE data (using the enterprise datalake via ESML AutoLake and ESML SDK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BUILD\n",
    "p_factory.create_dataset_scripts_from_template(overwrite_if_exists=True) # Do this once, then edit them manually. overwrite_if_exists=False is DEFAULT\n",
    "batch_pipeline = p_factory.create_batch_pipeline(esml_pipeline_types.IN_2_GOLD) # Creates pipeline from template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN\n",
    "pipeline_run = p_factory.execute_pipeline(batch_pipeline)\n",
    "pipeline_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUBLISH\n",
    "published_pipeline, endpoint = p_factory.publish_pipeline(batch_pipeline) # \"_1\" is optional    to create a NEW pipeline with 0 history, not ADD version to existing pipe & endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN_2_GOLD only - TRAIN...or refined data for Power BI\n",
    "- If just wanting to refine data to GOLD, before TRAIN-step, `or just to prep data for a Power BI report (No ML involved)`\n",
    "- Scenario: You want to refine data from \"IN_2_GOLD\" with an easy way to READ/WRITE data (using the enterprise datalake via ESML AutoLake and ESML SDK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.inference_mode = True\n",
    "## BUILD\n",
    "p_factory.create_dataset_scripts_from_template(overwrite_if_exists=False) # Do this once, then edit them manually. overwrite_if_exists=False is DEFAULT\n",
    "batch_pipeline = p_factory.create_batch_pipeline(esml_pipeline_types.IN_2_GOLD) # Creates pipeline from template\n",
    "\n",
    "## RUN\n",
    "pipeline_run = p_factory.execute_pipeline(batch_pipeline)\n",
    "pipeline_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change environment \n",
    "- Other curated\n",
    "- Non-curated, CONDA AutoML\n",
    "- https://docs.microsoft.com/en-us/azure/machine-learning/resource-curated-environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../azure-enterprise-scale-ml/esml/common/\")\n",
    "from esml import ESMLProject\n",
    "from baselayer_azure_ml_pipeline import ESMLPipelineFactory, esml_pipeline_types\n",
    "\n",
    "p = ESMLProject()\n",
    "p.inference_mode = True\n",
    "p.active_model = 11 # Diabetes\n",
    "p_factory = ESMLPipelineFactory(p)\n",
    "\n",
    "scoring_date = '1000-01-01 10:35:01.243860' # \n",
    "p_factory.batch_pipeline_parameters[1].default_value = scoring_date # overrides ESMLProject.date_scoring_folder.\n",
    "p_factory.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other curated - change environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p_factory.use_curated_automl_environment = True\n",
    "p_factory.environment_name = \"AzureML-AutoML-DNN\" # Training[ \"AzureML-AutoML\", \"AzureML-lightgbm-3.2-ubuntu18.04-py37-cpu\"]  Inference[\"AzureML-sklearn-0.24.1-ubuntu18.04-py37-cpu-inference\",]\n",
    "\n",
    "## BUILD\n",
    "p_factory.create_dataset_scripts_from_template(overwrite_if_exists=False) # Do this once, then edit them manually. overwrite_if_exists=False is DEFAULT\n",
    "batch_pipeline = p_factory.create_batch_pipeline(pipeline_type=esml_pipeline_types.IN_2_GOLD_SCORING,same_compute_for_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-curated - custom conda/pip definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_factory.use_curated_automl_environment = False\n",
    "\n",
    "######### See here for environments: https://docs.microsoft.com/en-us/azure/machine-learning/resource-curated-environments\n",
    "\n",
    "######### ESML Defaults to the below CONDA, when use_curated_automl_environment = False \n",
    "#aml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\n",
    "    #conda_packages=['pandas','scikit-learn'], \n",
    "    #pip_packages=['azureml-sdk[automl]', 'pyarrow'])\n",
    "\n",
    "## BUILD\n",
    "p_factory.create_dataset_scripts_from_template(overwrite_if_exists=False) # Do this once, then edit them manually. overwrite_if_exists=False is DEFAULT\n",
    "batch_pipeline = p_factory.create_batch_pipeline(pipeline_type=esml_pipeline_types.IN_2_GOLD_SCORING,same_compute_for_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "pipeline_run = p_factory.execute_pipeline(batch_pipeline)\n",
    "pipeline_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit/Customize the ESML auto-generated pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using lake_settings.json with ESML version 1.4 - Models array support including LABEL\n",
      "Creates template step_files.py for user to edit at:\n",
      "Edit at ../../../2_A_aml_pipeline/4_inference/batch/M11/in2silver_ds01_diabetes.py\n",
      "Edit at ../../../2_A_aml_pipeline/4_inference/batch/M11/in2silver_ds02_other.py\n",
      "Edit at ../../../2_A_aml_pipeline/4_inference/batch/M11/silver_merged_2_gold.py\n",
      "Edit at ../../../2_A_aml_pipeline/4_inference/batch/M11/scoring_gold.py\n",
      "Edit at ../../../2_A_aml_pipeline/4_inference/batch/M11/train_split_and_register.py\n",
      "Edit at ../../../2_A_aml_pipeline/4_inference/batch/M11/train_manual.py\n",
      "Edit at ../../../2_A_aml_pipeline/4_inference/batch/M11/your_code/your_custom_code.py\n",
      "Using GEN2 as Datastore\n",
      "use_project_sp_2_mount: True\n",
      "ESML will auto-create a compute...\n",
      "Note: OVERRIDING enterprise performance settings with project specifics. (to change, set flag in 'dev_test_prod_settings.json' -> override_enterprise_settings_with_model_specific=False)\n",
      "Using a model specific cluster, per configuration in project specific settings, (the integer of 'model_number' is the base for the name)\n",
      "Note: OVERRIDING enterprise performance settings with project specifics. (to change, set flag in 'dev_test_prod_settings.json' -> override_enterprise_settings_with_model_specific=False)\n",
      "Found existing cluster p002-m11weu-dev for project and environment, using it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n",
      "image_build_compute = p002-m11weu-dev\n",
      "Reusing existing compute...\n",
      "create_gold_to_score_step: inference_mode=True\n",
      "<class 'azureml.pipeline.steps.python_script_step.PythonScriptStep'>\n",
      "IN 2 SILVER - ds01_diabetes\n",
      "<class 'azureml.pipeline.steps.python_script_step.PythonScriptStep'>\n",
      "IN 2 SILVER - ds02_other\n",
      "<class 'azureml.pipeline.steps.python_script_step.PythonScriptStep'>\n",
      "SILVER MERGED 2 GOLD\n",
      "<class 'azureml.pipeline.steps.python_script_step.PythonScriptStep'>\n",
      "SCORING GOLD\n",
      "<azureml.pipeline.core.pipeline.Pipeline object at 0x0000018BFF5F7D30>\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../azure-enterprise-scale-ml/esml/common/\")\n",
    "from esml import ESMLProject\n",
    "from baselayer_azure_ml_pipeline import ESMLPipelineFactory, esml_pipeline_types\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "p = ESMLProject()\n",
    "p.inference_mode = True\n",
    "p.active_model = 11 # Diabetes\n",
    "p_factory = ESMLPipelineFactory(p)\n",
    "scoring_date = '1000-01-01 10:35:01.243860' # \n",
    "p_factory.batch_pipeline_parameters[1].default_value = scoring_date # overrides ESMLProject.date_scoring_folder.\n",
    "#p.ws = p.get_workspace_from_config() \n",
    "\n",
    "p_factory.create_dataset_scripts_from_template(overwrite_if_exists=True) # Do this once, then edit them manually. overwrite_if_exists=False is DEFAULT\n",
    "batch_pipeline = p_factory.create_batch_pipeline(esml_pipeline_types.IN_2_GOLD_SCORING)\n",
    "\n",
    "\n",
    "#1) Get the ESML auto-generated steps\n",
    "step_array = p_factory.pipeline_steps_array\n",
    "\n",
    "for step in step_array:\n",
    "    t1 = type(step) # https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.python_script_step.pythonscriptstep?view=azure-ml-py\n",
    "    print(str(t1))\n",
    "    print(step.name)\n",
    "\n",
    "#2)  EDIT pipeline as you wish...change compute, environments, add steps, remove steps, etc.\n",
    "\n",
    "pipeline = Pipeline(workspace = p.ws, steps=step_array) # 3) Create a pipelin\n",
    "print(pipeline)\n",
    "\n",
    "#4) RUN the pipeline as below:\n",
    "\n",
    "#pipeline_run = p_factory.execute_pipeline(pipeline)\n",
    "#pipeline_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOLD_SCORING\n",
    "- If just wanting to SCORE, with best trained model in Azure ML Studio\n",
    "- Scenario: Another service, Databricks or Azyre Synapse, refined data fron IN_2_GOLD\", you just want to use Azure ML for scoring\n",
    "    - Azure ML benefits:\n",
    "        - Get lineage via Azure ML Datasets and Azure ML Model linage. \n",
    "        - Host the batch scoring pipleine in your Azure ML Studio\n",
    "    - EMSL benefits\n",
    "        - ESMLPipelinefactory + ESML AutoLake design, ESML extra logging & lineage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BUILD\n",
    "p_factory.create_dataset_scripts_from_template(overwrite_if_exists=False) # Do this once, then edit them manually. overwrite_if_exists=False is DEFAULT\n",
    "batch_pipeline = p_factory.create_batch_pipeline(esml_pipeline_types.GOLD_SCORING) # Creates pipeline from template\n",
    "\n",
    "## RUN\n",
    "pipeline_run = p_factory.execute_pipeline(batch_pipeline)\n",
    "pipeline_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b) When satisfied - `PUBLISH` pipeline (or rebuild and publish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step IN 2 SILVER - ds01_diabetes [e949f734][d27f96d1-9a05-4112-baca-933578ddb4e7], (This step is eligible to reuse a previous run's output)\n",
      "Created step IN 2 SILVER - ds02_other [5a5aeff1][fdbc753c-0588-4dd2-a558-85f657f6dbea], (This step is eligible to reuse a previous run's output)\n",
      "Created step SILVER MERGED 2 GOLD [805c9c01][a628abdb-95db-4f02-ae7f-d60658c593ab], (This step is eligible to reuse a previous run's output)\n",
      "Created step SCORING GOLD [4207f138][9749d505-9b31-4b94-8342-c60ac7053e48], (This step is eligible to reuse a previous run's output)\n",
      "pub_pipe.name 10_titanic_model_clas_pipe_IN_2_GOLD_SCORING_EP_1\n",
      "pub_pipe.id 6c3c51e9-df26-4373-ae37-0a31a3616ec0\n"
     ]
    }
   ],
   "source": [
    "# PUBLISH\n",
    "published_pipeline, endpoint = p_factory.publish_pipeline(batch_pipeline, \"_1\") # \"_1\" is optional    to create a NEW pipeline with 0 history, not ADD version to existing pipe & endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c) `REBUILD` and `REPUBLISH` on `SAME endpoint` but new version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did NOT overwrite script-files with template-files such as 'scoring_gold.py', since overwrite_if_exists=False\n",
      "Using GEN2 as Datastore\n",
      "use_project_sp_2_mount: True\n",
      "ESML will auto-create a compute...\n",
      "Using a model specific cluster, per configuration in project specific settings, (the integer of 'model_number' is the base for the name)\n",
      "Note: OVERRIDING enterprise performance settings with project specifics. (to change, set flag in 'dev_test_prod_settings.json' -> override_enterprise_settings_with_model_specific=False)\n",
      "Found existing cluster p002-m11weu-dev for project and environment, using it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n",
      "image_build_compute = p002-m11weu-dev\n",
      "Reusing existing compute...\n",
      "create_gold_to_score_step: inference_mode=True\n",
      "Created step IN 2 SILVER - ds01_diabetes [de822f4f][5a850ba6-ab47-4f1e-81a1-fb76c82eda75], (This step will run and generate new outputs)\n",
      "Created step IN 2 SILVER - ds02_other [8d17c0d8][535d5263-afd2-4732-8314-930b55f1cbcf], (This step will run and generate new outputs)\n",
      "Created step SILVER MERGED 2 GOLD [4539ab81][a31beeca-f12b-4c1f-b696-473cfec5ff8f], (This step will run and generate new outputs)\n",
      "Created step SCORING GOLD [b293e919][ffff7d24-1ac7-48a3-b991-17a5537eca1f], (This step will run and generate new outputs)\n",
      "pub_pipe.name 11_diabetes_model_reg_pipe_IN_2_GOLD_SCORING_EP_6\n",
      "pub_pipe.id 01274209-95e8-4ffb-9b95-9bfb1c865b28\n"
     ]
    }
   ],
   "source": [
    "# REBUILD - if you haven't runned the above cell, uncommen below:\n",
    "p_factory.create_dataset_scripts_from_template(overwrite_if_exists=False) # overwrite_if_exists=False is default\n",
    "batch_pipeline = p_factory.create_batch_pipeline(pipeline_type=esml_pipeline_types.IN_2_GOLD_SCORING, same_compute_for_all=True, cpu_gpu_databricks=\"cpu\", allow_reuse=False)  # Gets workspace, connects to lake, creates pipeline.\n",
    "\n",
    "# PUBLISH\n",
    "published_pipeline, endpoint = p_factory.publish_pipeline(batch_pipeline, \"_6\") #  ADD version to existing pipeline & endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In AZURE DATA FACTORY - This is the ID you need, if using PRIVATE LINK, private Azure ML workspace.\n",
      "-You need PIPELINE id, not pipeline ENDPOINT ID ( since cannot be chosen in Azure data factory if private Azure ML)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'7fdef475-f619-4f40-8ca1-0001a0166db2'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"In AZURE DATA FACTORY - This is the ID you need, if using PRIVATE LINK, private Azure ML workspace.\")\n",
    "print(\"-You need PIPELINE id, not pipeline ENDPOINT ID ( since cannot be chosen in Azure data factory if private Azure ML)\")\n",
    "published_pipeline.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) `CONSUME` pipeline: HowTo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a) Consume from `Azure Data factory - BATCH_SCORE Pipeline activity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2) Fetch scored data: Below needed for Azure Data factory PIPELINE activity (Pipeline OR Endpoint. Choose the latter\n",
      "- Endpoint ID\n",
      "Endpoint ID:  01274209-95e8-4ffb-9b95-9bfb1c865b28\n",
      "Endpoint Name:  11_diabetes_model_reg_pipe_IN_2_GOLD_SCORING_EP_6\n",
      "Experiment name:  11_diabetes_model_reg_pipe_IN_2_GOLD_SCORING\n"
     ]
    }
   ],
   "source": [
    "print(\"2) Fetch scored data: Below needed for Azure Data factory PIPELINE activity (Pipeline OR Endpoint. Choose the latter\") \n",
    "print(\"- PIPELINE ID:  {}\".format(published_pipeline.id))\n",
    "print (\"- Endpoint ID\")\n",
    "print(\"Endpoint ID:  {}\".format(endpoint.id))\n",
    "print(\"Endpoint Name:  {}\".format(endpoint.name))\n",
    "print(\"Experiment name:  {}\".format(p_factory.experiment_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a) Get MetaData about scored data: `gold_scored_runinfo` Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read this meta-dataset from ADF: projects/project002/11_diabetes_model_reg/inference/active/gold_scored_runinfo/last_gold_run.csv\n",
      "- To get the column 'scored_gold_path' which points to the scored-data:\n",
      "projects/project002/11_diabetes_model_reg/inference/1/scored/dev/1000/01/01/e2635d17-4812-46e8-9e2b-4178bd4b9215/*.parquet\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import PipelineRun\n",
    "\n",
    "# 1st you need a \"Post scoring\" activity, to get metadata of \"scored_gold_path\" from \"last_gold_run.csv\"\n",
    "ds1 = Dataset.get_by_name(workspace = p.ws, name =  p.dataset_gold_scored_runinfo_name_azure)\n",
    "run_id = ds1.to_pandas_dataframe().iloc[0][\"pipeline_run_id\"] # ['pipeline_run_id', 'scored_gold_path', 'date_in_parameter', 'date_at_pipeline_run','model_version'])\n",
    "scored_gold_path = ds1.to_pandas_dataframe().iloc[0][\"scored_gold_path\"]\n",
    "\n",
    "print(\"Read this meta-dataset from ADF: {}/last_gold_run.csv\".format(p.path_inference_gold_scored_runinfo))\n",
    "print(\"- To get the column 'scored_gold_path' which points to the scored-data:\")\n",
    "print(\"The scored data will be stored as below, except DateFolders, that will be dynamic, not /1000/01/01\")\n",
    "print(\"\")\n",
    "print(\"{}*.parquet\".format(scored_gold_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b) Consume from `from PYTHON`\n",
    "- Run a pipeline endpoint (`Python SDK` call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'17d33ab3-aa02-40fe-be8c-8a94f96fa36b'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.pipeline.core import PipelineEndpoint\n",
    "pipeline_endpoint = PipelineEndpoint.get(workspace=p.ws, name=p_factory.name_batch_pipeline_endpoint +\"_6\")\n",
    "pipeline_run_sdk = pipeline_endpoint.submit(p_factory.experiment_name)\n",
    "pipeline_run_sdk.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Preparing'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_run_sdk.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c) Consume from `from REST / PYTHON`\n",
    "- Run via REST call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PublishedPipeline,PipelineEndpoint,PipelineRun\n",
    "import requests\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication # InteractiveLoginAuthentication, AzureCliAuthentication\n",
    "\n",
    "sp = p.get_authenticaion_header_sp()\n",
    "auth_header = sp.get_authentication_header()\n",
    "date_folder = str(p.date_scoring_folder)\n",
    "pipeline_endpoint = PipelineEndpoint.get(workspace=p.ws, name=p_factory.name_batch_pipeline_endpoint)\n",
    "\n",
    "response = requests.post(pipeline_endpoint.endpoint,\n",
    "                         headers=auth_header,\n",
    "                         json={\"ExperimentName\": p_factory.experiment_name,\n",
    "                               \"ParameterAssignments\": {\n",
    "                                     \"esml_inference_model_version\": p.inferenceModelVersion,\n",
    "                                     \"esml_scoring_folder_date\": date_folder\n",
    "                                     }\n",
    "                              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response.raise_for_status()\n",
    "except Exception:    \n",
    "    raise Exception(\"Received bad response from the endpoint: {}\\n\"\n",
    "                    \"Response Code: {}\\n\"\n",
    "                    \"Headers: {}\\n\"\n",
    "                    \"Content: {}\".format(pipeline_endpoint.endpoint, response.status_code, response.headers, response.content))\n",
    "\n",
    "run_id = response.json().get('Id')\n",
    "print('Submitted pipeline run: ', run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View status from REST call, via SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PublishedPipeline,PipelineEndpoint,PipelineRun\n",
    "published_pipeline_run = PipelineRun(p.ws.experiments[p_factory.experiment_name], run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline_run.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `WHO is the caller, usually?` (Azure Data factory, Azure Devops)` - that sends PARAMETERS and WHY?` \n",
    "### Q: Why? \n",
    "- A: To use same DEV scoring pipeline, with either different data to be scored `daily scoring`, or `different model-version SAME day` to score with.\n",
    "- A: To have \"environment parameters (dev,test,prod) we can instatiate a ESMLProject what knows the lake, workspace, makes it easy to create 3 pipelines for dev,test,prod\n",
    "    - And data, if 1 LAKE or 3 LAKES (dev,test,prod), they all have data-folders \"dev,test,prod\"\n",
    "\n",
    "### Who gives input?\n",
    "- A) Azure Devops (CI/CD) will trigger TRAIN pipeline, that will end with creating this BATCH SCORING, with \n",
    "    - 2 parameters (`esml_environment, esml_inference_model_version`), to CREATE/UPDATE the BATCH pipeline with newly trained model\n",
    "    - 1 dummy (`esml_scoring_folder_date`) to test BATCH SCORING after creation.\n",
    "- B) Azure Datafactory (read from source, writes as .csv or .parquet to IN-folder), and will trigger BATCH SCORING with:\n",
    "    - 2 PIPELINE parameters (`esml_inference_model_version, esml_scoring_folder_date`), to read IN-DATA to be scored. Usually \"todays\" esml_scoring_folder_date\n",
    "    - Note: To solve \"many scorings same day\", a \"run.id\" folder is created before the actual data.parquet\n",
    "    - Note: `*esml_environment` is not really needed post creation - since we already created the pipleine in DEV, `locked and loaded`\n",
    "\n",
    "### Who needs `scored_data` and HOW to get it? META data:\n",
    "- `Azure Datafactory` can read meta data of `last scored GOLD`, to get datalake-path of SCORED_GOLD - can then \"`write back scored data`\" to source, or another `system`\n",
    "    - See next cells \"`Get previous RUN and PIPELINE via `ESML` metadata`\"\n",
    "- `Power BI` can read the meta-data to fetch `last scored GOLD` directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get previous RUN and PIPELINE via `ESML` metadata\n",
    "- How to get path of `scored_gold_path` and how to see the actual `pipeline run`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method format of str object at 0x0000018B93D9CB10>\n",
      "pipeline_run_id: e2635d17-4812-46e8-9e2b-4178bd4b9215\n",
      "scored_gold_path: 'projects/project002/11_diabetes_model_reg/inference/1/scored/dev/1000/01/01/e2635d17-4812-46e8-9e2b-4178bd4b9215/'\n",
      "\n",
      "Fetched RUN object Run(Experiment: 11_diabetes_model_reg_pipe_IN_2_GOLD_SCORING,\n",
      "Id: e2635d17-4812-46e8-9e2b-4178bd4b9215,\n",
      "Type: azureml.PipelineRun,\n",
      "Status: Completed)\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core import Experiment\n",
    "from  azureml.pipeline.core import PipelineRun\n",
    "\n",
    "# Get \"Pipeline run\" info, for tghe most recent \"latest scored gold\"\n",
    "ds1 = Dataset.get_by_name(workspace = p.ws, name =  p.dataset_gold_scored_runinfo_name_azure)\n",
    "run_id = ds1.to_pandas_dataframe().iloc[0][\"pipeline_run_id\"] # ['pipeline_run_id', 'scored_gold_path', 'date_in_parameter', 'date_at_pipeline_run','model_version'])\n",
    "scored_gold_path = ds1.to_pandas_dataframe().iloc[0][\"scored_gold_path\"]\n",
    "\n",
    "print(\"dataset_gold_scored_runinfo, location: {}\".format)\n",
    "print(\"pipeline_run_id: {}\".format(run_id))\n",
    "print(\"scored_gold_path: '{}'\".format(scored_gold_path))\n",
    "\n",
    "experiment = Experiment(workspace=p.ws, name=p_factory.experiment_name)\n",
    "remote_run = PipelineRun(experiment=experiment, run_id=run_id)\n",
    "print(\"\\nFetched RUN object {}\".format(remote_run))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `What can you configure?` (parameters, step compute, custom code)\n",
    "## 1) Configure: Parameters\n",
    "- Pipeline parameters: scoring_date, model_version\n",
    "    - Why: To dynamically select different data & model to score with, with same pipeline/reuse.\n",
    "    - Who: Azure data factory can dynamically set these, and call AML pipline\n",
    "- Pipeline parameters (model specific): target_column_name\n",
    "    - Why: To merge datasets to GOLD.\n",
    "print(\"Model version (pipeline parameter): {}\".format(p_factory.batch_pipeline_parameters[0].default_value))\n",
    "print(\" - This default value is set from ESMLProject settings: {}\".format(p.inferenceModelVersion))\n",
    "print(\"Scoring datetime: {}\".format(p_factory.batch_pipeline_parameters[1].default_value))\n",
    "print(\" - This default value is set from ESMLProject settings: {}\".format(str(p.date_scoring_folder)))\n",
    "# Optional parameters to READ or SET\n",
    "#parameters[2].name: parameters[2].default_value, # esml_optional_unique_scoring_folder \n",
    "#parameters[3].name: parameters[3].default_value # par_esml_dev_test_prod\n",
    "## 2) Configure: Compute & Environment (via ESML config or inject your own)\n",
    "- `Different compute per step OR samee for all` [\"cpu\",\"gpu\", \"databricks\"], based on your ESML environment (dev,test,prod) compute settings, and Dataset properties.\n",
    " - A) `Different compute for all steps`\n",
    "\n",
    "        -  if(d.cpu_gpu_databricks == \"cpu\"):\n",
    "        -       compute, runconfig = self.init_cpu_environment()\n",
    "        -  elif(d.cpu_gpu_databricks == \"databricks\"):\n",
    "        -     compute, runconfig = self.init_databricks_environment()\n",
    "        -  elif(d.cpu_gpu_databricks == \"gpu\"):\n",
    "        -       compute, runconfig = self.init_gpu_environment()\n",
    "- B) `Same compute for all`: For the full pipeline, is the DEFAULT behaviour.\n",
    "\n",
    "        - def `create_batch_scoring_pipe(self, `same_compute_for_all=True`, `cpu_gpu_databricks=\"cpu\")`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2f0f778a4495e689b30073b7a599e6a826d304e8985d11475b75364c935a444d"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('azure_automl_esml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
